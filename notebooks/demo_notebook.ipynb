{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70634205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32bc71c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.path.abspath(\"..\")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "393a84d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.tfidf_cosine import TFIDFCosineSimilarity\n",
    "from models.okapi_bm25 import BM25Similarity\n",
    "from models.jaccard import JaccardSimilarity\n",
    "from models.sbert import SentenceBERTSimilarity\n",
    "from models.bertscore import BERTScoreSimilarity\n",
    "from evaluation.metrics import evaluate_model\n",
    "from evaluation.analyzer import generate_analysis_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37d5a1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "004f91f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \"\"\"Load the Philippine legal dataset\"\"\"\n",
    "    with open('../dataset/philippine-legal-dataset.json', 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "752fcdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models():\n",
    "    \"\"\"Initialize all similarity models\"\"\"\n",
    "    return {\n",
    "        'TF-IDF Cosine': TFIDFCosineSimilarity(use_dimensionality_reduction=True, n_components=50),\n",
    "        'BM25': BM25Similarity(),\n",
    "        'Jaccard': JaccardSimilarity(),\n",
    "        'Sentence BERT': SentenceBERTSimilarity(model_name='all-mpnet-base-v2'),\n",
    "        'BERTScore': BERTScoreSimilarity(model_type='distilbert-base-uncased')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "588a931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_demo():\n",
    "    \"\"\"Interactive widgets for similarity testing\"\"\"\n",
    "    \n",
    "    # Load data and models\n",
    "    dataset = load_dataset()\n",
    "    models = initialize_models()\n",
    "    \n",
    "    # Create widgets\n",
    "    query_widget = widgets.Textarea(\n",
    "        value=\"What is the function of the Presidential Electoral Tribunal?\",\n",
    "        placeholder=\"Enter your query here...\",\n",
    "        description=\"Query:\",\n",
    "        layout=widgets.Layout(width='100%', height='80px')\n",
    "    )\n",
    "    \n",
    "    model_widget = widgets.Dropdown(\n",
    "        options=list(models.keys()),\n",
    "        value='Sentence BERT',\n",
    "        description=\"Model:\",\n",
    "    )\n",
    "    \n",
    "    example_widget = widgets.Dropdown(\n",
    "        options=[(f\"Example {i+1}: {item['query'][:50]}...\", i) \n",
    "                for i, item in enumerate(dataset[:10])],\n",
    "        description=\"Examples:\",\n",
    "    )\n",
    "    \n",
    "    def load_example(change):\n",
    "        if change['new'] is not None:\n",
    "            example = dataset[change['new']]\n",
    "            query_widget.value = example['query']\n",
    "    \n",
    "    example_widget.observe(load_example, names='value')\n",
    "    \n",
    "    def run_similarity(b):\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            query = query_widget.value\n",
    "            model_name = model_widget.value\n",
    "            model = models[model_name]\n",
    "            \n",
    "            # Find matching example or use custom passages\n",
    "            example = None\n",
    "            for item in dataset:\n",
    "                if item['query'] == query:\n",
    "                    example = item\n",
    "                    break\n",
    "            \n",
    "            if example:\n",
    "                passages = [p['text'] for p in example['passages']]\n",
    "                labels = [p['relevant'] for p in example['passages']]\n",
    "                tags = [p['label'] for p in example['passages']]\n",
    "            else:\n",
    "                # Use first example's passages for demo\n",
    "                example = dataset[0]\n",
    "                passages = [p['text'] for p in example['passages']]\n",
    "                labels = [p['relevant'] for p in example['passages']]\n",
    "                tags = [p['label'] for p in example['passages']]\n",
    "            \n",
    "            # Calculate similarities\n",
    "            scores = model.rank(query, passages)\n",
    "            sorted_indices = np.argsort(scores)[::-1]\n",
    "            \n",
    "            # Create results DataFrame\n",
    "            results_df = pd.DataFrame({\n",
    "                'Rank': range(1, len(passages) + 1),\n",
    "                'Passage': [passages[i] for i in sorted_indices],\n",
    "                'Score': [scores[i] for i in sorted_indices],\n",
    "                'Relevant': [labels[i] for i in sorted_indices],\n",
    "                'Label': [tags[i] for i in sorted_indices]\n",
    "            })\n",
    "            \n",
    "            # Display results\n",
    "            print(f\"Query: {query}\")\n",
    "            print(f\"Model: {model_name}\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            # Show top 3 results\n",
    "            for i in range(min(3, len(results_df))):\n",
    "                row = results_df.iloc[i]\n",
    "                status = \"✅ Relevant\" if row['Relevant'] else \"❌ Irrelevant\"\n",
    "                print(f\"\\nRank {row['Rank']}: {status} (Score: {row['Score']:.4f})\")\n",
    "                print(f\"Label: {row['Label']}\")\n",
    "                print(f\"Passage: {row['Passage'][:200]}...\")\n",
    "            \n",
    "            # Create visualization\n",
    "            fig = make_subplots(\n",
    "                rows=2, cols=2,\n",
    "                subplot_titles=('Similarity Scores', 'Relevance Distribution', \n",
    "                              'Score vs Relevance', 'Label Distribution'),\n",
    "                specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "                       [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "            )\n",
    "            \n",
    "            # Bar chart of scores\n",
    "            colors = ['green' if rel else 'red' for rel in results_df['Relevant']]\n",
    "            fig.add_trace(\n",
    "                go.Bar(x=results_df['Rank'], y=results_df['Score'], \n",
    "                      marker_color=colors, name='Similarity Score'),\n",
    "                row=1, col=1\n",
    "            )\n",
    "            \n",
    "            # Relevance pie chart\n",
    "            relevance_counts = results_df['Relevant'].value_counts()\n",
    "            fig.add_trace(\n",
    "                go.Pie(labels=['Irrelevant', 'Relevant'], \n",
    "                      values=[relevance_counts.get(False, 0), relevance_counts.get(True, 0)],\n",
    "                      name='Relevance'),\n",
    "                row=1, col=2\n",
    "            )\n",
    "            \n",
    "            # Scatter plot: Score vs Relevance\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=results_df['Score'], y=results_df['Relevant'].astype(int),\n",
    "                          mode='markers', marker=dict(size=10, color=colors),\n",
    "                          name='Score vs Relevance'),\n",
    "                row=2, col=1\n",
    "            )\n",
    "            \n",
    "            # Label distribution\n",
    "            label_counts = results_df['Label'].value_counts()\n",
    "            fig.add_trace(\n",
    "                go.Bar(x=label_counts.index, y=label_counts.values,\n",
    "                      name='Label Distribution'),\n",
    "                row=2, col=2\n",
    "            )\n",
    "            \n",
    "            fig.update_layout(height=800, showlegend=False, \n",
    "                            title_text=f\"Similarity Analysis: {model_name}\")\n",
    "            fig.show()\n",
    "    \n",
    "    button = widgets.Button(description=\"Run Similarity Analysis\", \n",
    "                           button_style='primary')\n",
    "    button.on_click(run_similarity)\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    # Display widgets\n",
    "    display(widgets.VBox([\n",
    "        widgets.HTML(\"<h2>Interactive Text Similarity Demo</h2>\"),\n",
    "        example_widget,\n",
    "        query_widget,\n",
    "        model_widget,\n",
    "        button,\n",
    "        output\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "265b29f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_comparison():\n",
    "    \"\"\"Compare performance of all models\"\"\"\n",
    "    \n",
    "    dataset = load_dataset()\n",
    "    models = initialize_models()\n",
    "    \n",
    "    print(\"Running performance comparison...\")\n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"Evaluating {name}...\")\n",
    "        metrics = evaluate_model(model, dataset)\n",
    "        results[name] = metrics\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    df = pd.DataFrame(results).T\n",
    "    \n",
    "    # Display results table\n",
    "    display(HTML(\"<h3>Performance Comparison</h3>\"))\n",
    "    display(df.round(4))\n",
    "    \n",
    "    # Create comparison visualizations\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Precision@1', 'Precision@3', 'Recall@3', 'MRR'),\n",
    "    )\n",
    "    \n",
    "    metrics_to_plot = ['Precision@1', 'Precision@3', 'Recall@3', 'MRR']\n",
    "    positions = [(1,1), (1,2), (2,1), (2,2)]\n",
    "    \n",
    "    for metric, (row, col) in zip(metrics_to_plot, positions):\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=df.index, y=df[metric], name=metric),\n",
    "            row=row, col=col\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(height=600, showlegend=False, \n",
    "                     title_text=\"Model Performance Comparison\")\n",
    "    fig.show()\n",
    "    \n",
    "    # Runtime and memory comparison\n",
    "    fig2 = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=('Runtime (seconds)', 'Peak Memory (KB)'),\n",
    "    )\n",
    "    \n",
    "    fig2.add_trace(\n",
    "        go.Bar(x=df.index, y=df['Runtime (in seconds)'], name='Runtime'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig2.add_trace(\n",
    "        go.Bar(x=df.index, y=df['Peak Memory (in KB)'], name='Memory'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig2.update_layout(height=400, showlegend=False,\n",
    "                      title_text=\"Resource Usage Comparison\")\n",
    "    fig2.show()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a5cd68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_monitoring_dashboard():\n",
    "    \"\"\"Live monitoring dashboard\"\"\"\n",
    "    \n",
    "    dataset = load_dataset()\n",
    "    models = initialize_models()\n",
    "    \n",
    "    # Create monitoring widgets\n",
    "    model_select = widgets.SelectMultiple(\n",
    "        options=list(models.keys()),\n",
    "        value=['Sentence BERT', 'BM25'],\n",
    "        description=\"Models:\",\n",
    "        rows=5\n",
    "    )\n",
    "    \n",
    "    metric_select = widgets.Dropdown(\n",
    "        options=['Precision@1', 'Precision@3', 'Recall@3', 'MRR'],\n",
    "        value='Precision@1',\n",
    "        description=\"Metric:\"\n",
    "    )\n",
    "    \n",
    "    def update_monitoring(change):\n",
    "        with monitoring_output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            selected_models = model_select.value\n",
    "            selected_metric = metric_select.value\n",
    "            \n",
    "            if not selected_models:\n",
    "                print(\"Please select at least one model\")\n",
    "                return\n",
    "            \n",
    "            # Evaluate selected models\n",
    "            results = {}\n",
    "            for model_name in selected_models:\n",
    "                model = models[model_name]\n",
    "                metrics = evaluate_model(model, dataset)\n",
    "                results[model_name] = metrics\n",
    "            \n",
    "            # Create real-time visualization\n",
    "            df = pd.DataFrame(results).T\n",
    "            \n",
    "            fig = go.Figure()\n",
    "            fig.add_trace(go.Bar(\n",
    "                x=df.index,\n",
    "                y=df[selected_metric],\n",
    "                text=df[selected_metric].round(4),\n",
    "                textposition='auto',\n",
    "            ))\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title=f\"Live Performance Monitoring: {selected_metric}\",\n",
    "                xaxis_title=\"Models\",\n",
    "                yaxis_title=selected_metric,\n",
    "                height=400\n",
    "            )\n",
    "            \n",
    "            fig.show()\n",
    "            \n",
    "            # Show detailed metrics table\n",
    "            display(HTML(f\"<h4>Detailed Metrics</h4>\"))\n",
    "            display(df.round(4))\n",
    "    \n",
    "    model_select.observe(update_monitoring, names='value')\n",
    "    metric_select.observe(update_monitoring, names='value')\n",
    "    \n",
    "    monitoring_output = widgets.Output()\n",
    "    \n",
    "    display(widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Live Performance Monitoring</h3>\"),\n",
    "        widgets.HBox([model_select, metric_select]),\n",
    "        monitoring_output\n",
    "    ]))\n",
    "    \n",
    "    # Initial update\n",
    "    update_monitoring(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500dfcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Text Similarity Evaluation Demo\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Interactive demo\n",
    "    create_interactive_demo()\n",
    "    \n",
    "    # Performance comparison\n",
    "    performance_comparison()\n",
    "    \n",
    "    # Live monitoring\n",
    "    live_monitoring_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca8683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'hi'"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
